{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get credentials\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    %run ../Introduction.ipynb\n",
    "    \n",
    "from datetime import datetime\n",
    "from sentenai import Sentenai\n",
    "import pandas as pd\n",
    "sentenai = Sentenai(host=host, port=port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Stream Databases\n",
    "\n",
    "Stream databases are the building block of data sets in Sentenai. A Stream database represent something as simple as a single data feed, or as complex as an entire production line on a factory floor. Data streams in a stream database are organized into a tree-like hierarchy of relationships. This hierarchy is purely organizational in nature, but splitting complex flat data sets into a hierarchical organization can have substantial performance benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a flat dataset called \"Activity\" like you'd typically see in a relational database:\n",
    "\n",
    "| Timestamp | Location | Sensor | Temperature | Humidity\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 2020-01-01T00:00:00Z | Boston | 53 | 48 | .83\n",
    "| 2020-01-01T00:00:00Z | Providence | 48 | 43 | .55\n",
    "\n",
    "\n",
    "In Sentenai you might instead arrange your dataset:\n",
    "```\n",
    "- Activity\n",
    "  - Boston\n",
    "    - 53\n",
    "      - Temperature\n",
    "      - Humidity\n",
    "  - Providence\n",
    "    - 48\n",
    "      - Temperature\n",
    "      - Humidity\n",
    "```\n",
    "\n",
    "This organization implies there are natural filters you'd want to apply:\n",
    "\n",
    "`Activity/Boston/53/Temperature` is equivalent to filtering a table on Location and Sensor id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing data in a Stream Database\n",
    "\n",
    "Stream databases feature transactional data logging. Any streams within the stream graph can be updated in a single transaction, provided the update applies to the same time interval across all updated streams.\n",
    "\n",
    "So if we initialize a new database `test-2':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sentenai.init('test-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update the entire database with each insertion without requiring any up-front schema declaration of stream instantiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Log is currently disabled for single-node installations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLog is currently disabled for single-node installations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m db\u001b[38;5;241m.\u001b[39mlog[datetime(\u001b[38;5;241m2020\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m): datetime(\u001b[38;5;241m2020\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m): \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate-1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBoston\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m53\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     }\n\u001b[1;32m     15\u001b[0m }\n",
      "\u001b[0;31mException\u001b[0m: Log is currently disabled for single-node installations"
     ]
    }
   ],
   "source": [
    "raise Exception(\"Log is currently disabled for single-node installations\")\n",
    "db.log[datetime(2020, 1, 1): datetime(2020, 1, 1, 1): 'update-1'] = {\n",
    "    'Boston': {\n",
    "        '53': {\n",
    "            'Temperature': 48.0,\n",
    "            'Humidity': 0.83,\n",
    "        },\n",
    "    },\n",
    "    'Providence': {\n",
    "        '48': {\n",
    "            'Temperature': 43.0,\n",
    "            'Humidity': 0.55,\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also can create indexes manually and use `insert` to add new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual creation\n",
    "db['Boston', '53', 'Temperature'] = float\n",
    "db['Boston', '53', 'Temperature'].insert([{\n",
    "    'start': datetime(2020, 1, 1),\n",
    "    'end': datetime(2020, 1, 1, 1),\n",
    "    'value': 48.0\n",
    "}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can use lists of events to automatically figure out an index. Note that this overwrites previous data and re-detects the index type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 134.25 values/s]\n"
     ]
    }
   ],
   "source": [
    "db['Boston', '53', 'Humidity'] = [{\n",
    "    'start': datetime(2020, 1, 1),\n",
    "    'end': datetime(2020, 1, 1, 1),\n",
    "    'value': 0.83\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can use a dataframe to build out all the contents of part of the database at once. Note that an additional `event`-typed index is created at the path `Providence/48`, which can be useful for keeping track of common intervals across indexes that were added together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 106.00 values/s]\n"
     ]
    }
   ],
   "source": [
    "db['Providence', '48'] = pd.DataFrame([{\n",
    "    'start': datetime(2020, 1, 1),\n",
    "    'end': datetime(2020, 1, 1, 1),\n",
    "    'Temperature': 43.0,\n",
    "    'Humidity': 0.55\n",
    "}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've inserted our first update into the database's `log`, we can see for ourselves the structure using the `.graph` property of the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-2\n",
      "├── Boston\n",
      "│   └── 53\n",
      "│       ├── Humidity\n",
      "│       └── Temperature\n",
      "└── Providence\n",
      "    └── 48\n",
      "        ├── Humidity\n",
      "        └── Temperature\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db.graph().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node in this tree is a stream of either events or values. A stream of values is just a stream of events with a value attached, so in essence every node in the tree is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting bulk data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the basic pattern of insertion into the log with `[ start : (optional) end : (optional) id ]`, you can also stream large amounts of data over a single connection using Python's `with` keyword and a streaming context manager. To start up the streaming connection, type:\n",
    "```\n",
    "with db.log as log:\n",
    "```\n",
    "This creates a new streaming connection, called `log`, for us to use to insert data. This connection can actively manage asynchronous uploading and buffering data, so you can feel free to iterate over very large files without worrying about them being loaded fully into memory. Here's a basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Disabled for single-node",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDisabled for single-node\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m db\u001b[38;5;241m.\u001b[39mlog \u001b[38;5;28;01mas\u001b[39;00m log:\n",
      "\u001b[0;31mException\u001b[0m: Disabled for single-node"
     ]
    }
   ],
   "source": [
    "raise Exception(\"Disabled for single-node\")\n",
    "import random\n",
    "with db.log as log:\n",
    "    for i in range(50):\n",
    "        db.log[datetime(2020, 1, 1, 2, i) : : ] = {\n",
    "            'Boston': {'53': {\n",
    "                'Humidity' : random.random(),\n",
    "                'Temperature': random.random() * 100\n",
    "            }}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Properties of a Stream Database\n",
    "\n",
    "A stream database has several properties that are useful when writing programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-2\n"
     ]
    }
   ],
   "source": [
    "# Name\n",
    "print(db.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.datetime64('1970-01-01T00:00:00')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Origin\n",
    "db.origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Streams in a Stream Database\n",
    "\n",
    "So far we've seen how to manage data in a stream database, but we haven't yet seen how to work with that data.\n",
    "\n",
    "Stream databases are dict-like objects, so we can access streams in the same way we would access a value in a dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-2/Boston\n",
      "test-2/Providence\n"
     ]
    }
   ],
   "source": [
    "for key in db:\n",
    "    print(db[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.keys()` and `.items()` methods are also supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For paths that are multiple levels deep you can access them one of three ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-2/Boston/53\n"
     ]
    }
   ],
   "source": [
    "print(db['Boston']['53'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-2/Boston/53\n"
     ]
    }
   ],
   "source": [
    "print(db['Boston', '53'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-2/Boston/53\n"
     ]
    }
   ],
   "source": [
    "print(db['Boston/53'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These ways are entirely equivalent, but one may be preferable over the other depending on the situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Next chapter: Streams](Streams.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
